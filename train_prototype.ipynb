{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:41<00:00, 4150923.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import copy\n",
    "import math\n",
    "import argparse\n",
    "import numpy as np\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from easydict import EasyDict\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import unet_model\n",
    "\n",
    "# Define the transformation to apply to the dataset\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "# ])\n",
    "\n",
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "# train_set = torchvision.datasets.CIFAR10(\n",
    "    \n",
    "#     root='./data',\n",
    "#     train=True,\n",
    "#     download=True,\n",
    "#     transform=transform_train,\n",
    "# )\n",
    "\n",
    "# Download and load the CIFAR10 training dataset\n",
    "\n",
    "train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "# trainloader = torch.utils.data.DataLoader(train_set, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "# Download and load the CIFAR10 test dataset\n",
    "# test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "# testloader = torch.utils.data.DataLoader(test_set, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "# Define the classes for CIFAR10\n",
    "# classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "######## CHECK METADATA #########\n",
    "metadata = EasyDict(\n",
    "    {\n",
    "        \"image_size\": 32,\n",
    "        \"num_classes\": 10,\n",
    "        \"train_images\": 50000,\n",
    "        \"val_images\": 10000,\n",
    "        \"num_channels\": 3,\n",
    "    }\n",
    ")\n",
    "############# PARAMETERS #############\n",
    "DIFFUSION_STEPS = 100\n",
    "BATCH_SIZE = 32 # OR 128\n",
    "LEARNING_RATE = 0.0001\n",
    "EMA_W = 0.75 # Exponential Moving Average Weight\n",
    "EPOCHS = 100\n",
    "PRETRAINED_MODEL = \"\"\n",
    "SAVE_MODEL = True\n",
    "SAVE_MODEL_PATH = \"./saved_models/\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class loss_logger:\n",
    "    def __init__(self, max_steps):\n",
    "        self.max_steps = max_steps\n",
    "        self.loss = []\n",
    "        self.start_time = time()\n",
    "        self.ema_loss = None\n",
    "        self.ema_w = 0.9\n",
    "\n",
    "    def log(self, v, display=False):\n",
    "        self.loss.append(v)\n",
    "        if self.ema_loss is None:\n",
    "            self.ema_loss = v\n",
    "        else:\n",
    "            self.ema_loss = self.ema_w * self.ema_loss + (1 - self.ema_w) * v\n",
    "\n",
    "        if display:\n",
    "            print(\n",
    "                f\"Steps: {len(self.loss)}/{self.max_steps} \\t loss (ema): {self.ema_loss:.3f} \"\n",
    "                + f\"\\t Time elapsed: {(time() - self.start_time)/3600:.3f} hr\"\n",
    "            )\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "    model,\n",
    "    dataloader,\n",
    "    diffusion,\n",
    "    optimizer,\n",
    "    logger,\n",
    "    lrs,\n",
    "    ema_dict,\n",
    "    device,\n",
    "):\n",
    "    model.train()\n",
    "    for step, (images, labels) in enumerate(dataloader):\n",
    "        assert (images.max().item() <= 1) and (0 <= images.min().item())\n",
    "\n",
    "        # must use [-1, 1] pixel range for images\n",
    "        images, labels = (\n",
    "            2 * images.to(device) - 1,\n",
    "            None, # class_cond = False\n",
    "        )\n",
    "        t = torch.randint(diffusion.timesteps, (len(images),), dtype=torch.int64).to(\n",
    "            device\n",
    "        )\n",
    "        xt, eps = diffusion.sample_from_forward_process(images, t)\n",
    "        pred_eps = model(xt, t, y=labels)\n",
    "\n",
    "        loss = ((pred_eps - eps) ** 2).mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if lrs is not None:\n",
    "            lrs.step()\n",
    "\n",
    "        # update ema_dict\n",
    "        # if args.local_rank == 0:\n",
    "        new_dict = model.state_dict()\n",
    "        for (k, v) in ema_dict.items():\n",
    "            ema_dict[k] = (\n",
    "                EMA_W * ema_dict[k] + (1 - EMA_W) * new_dict[k]\n",
    "            )\n",
    "        logger.log(loss.item(), display=not step % 100)\n",
    "\n",
    "    return ema_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# # tensor = torch.tensor([1.0, 2.0, 3.0], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    torch.cuda.set_device(device)\n",
    "    torch.manual_seed(123)\n",
    "    np.random.seed(123)\n",
    "\n",
    "\n",
    "    attention_ds = []\n",
    "    attention_resolutions = \"32,16,8\"\n",
    "\n",
    "    for res in attention_resolutions.split(\",\"):\n",
    "        attention_ds.append(metadata.image_size // int(res))\n",
    "    \n",
    "    model = unet_model.UNetModel(image_size=metadata.image_size, \n",
    "                                in_channels=metadata.num_channels, \n",
    "                                out_channels=metadata.num_classes,\n",
    "                                model_channels = 64,\n",
    "                                channel_mult = (1, 2, 2, 2),\n",
    "                                num_res_blocks = 3,\n",
    "                                dropout = 0.1,\n",
    "                                num_classes=None,  # We're not using class labels\n",
    "                                use_checkpoint=False,\n",
    "                                use_fp16=False,\n",
    "                                num_heads=4,\n",
    "                                attention_resolutions=tuple(attention_ds),\n",
    "                                num_head_channels=64,\n",
    "                                num_heads_upsample=-1,\n",
    "                                use_scale_shift_norm=True,\n",
    "                                resblock_updown=True,\n",
    "                                use_new_attention_order=True\n",
    "                                ).to(device)\n",
    "    \n",
    "    diffusion_model = unet_model.GaussianDiffusion(DIFFUSION_STEPS, device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    if PRETRAINED_MODEL == \"\":\n",
    "        print(\"Training from scratch\")\n",
    "    else:\n",
    "        print(f\"Loading pretrained model from {PRETRAINED_MODEL}\")\n",
    "        model.load_state_dict(torch.load(PRETRAINED_MODEL))\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    print(f\"Training dataset loaded: Number of batches: {len(train_loader)}, Number of images: {len(train_set)}\")\n",
    "    logger = loss_logger(len(train_loader) * EPOCHS)\n",
    "\n",
    "    ema_dict = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        ema_dict = train_one_epoch(model, train_loader, diffusion_model, optimizer, logger, None, ema_dict, device)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch} completed\")\n",
    "            torch.save(ema_dict, f\"{SAVE_MODEL_PATH}ema_dict_{epoch}.pt\")\n",
    "            torch.save(model.state_dict(), f\"{SAVE_MODEL_PATH}model_{epoch}.pt\")\n",
    "            \n",
    "    return 0\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
